# Ontologia para Gerenciamento de Risco em Sistemas de IA

Este reposit√≥rio tem como finalidade abrigar a documenta√ß√£o, juntamente com a modelagem e constru√ß√£o de uma ontologia para gerenciamento de Riscos em Sistemas de IA com base nas principais normas e projetos de lei sobre o assunto, como NIST e EU AI Act.

# Estrutura


# üìÑ ORSD - Ontology Requirements Specification Document: Gerenciamento de Riscos de Intelig√™ncia Artificial
## Prop√≥sito (Purpose)
A ontologia √© projetada para fornecer uma representa√ß√£o formal e estruturada do conhecimento relacionado ao Gerenciamento de Riscos de Intelig√™ncia Artificial (IA), com o objetivo de auxiliar profissionais e organiza√ß√µes a identificar, avaliar, tratar e monitorar os riscos inerentes aos sistemas de IA. O objetivo √© promover a transpar√™ncia, a responsabilidade e a conformidade regulat√≥ria ao longo do ciclo de vida da IA.

## Escopo (Scope)
A ontologia abrange o conhecimento formal e pr√°tico relacionado ao Gerenciamento de Riscos de IA, incluindo:

- Defini√ß√£o e taxonomia de riscos de IA
- Defini√ß√£o de atores envolvidos no processo de avalia√ß√£o de riscos de IA
- Mapeamento de requisitos e processos para garantir o uso seguro de sistemas de IA

## Linguagens de Implementa√ß√£o (Implementation Languages)
A ontologia ser√° implementada usando OntoUML.

## Usu√°rios Finais Pretendidos (Intended End-Users)
1. Profissionais de Risco e Conformidade que supervisionam a implanta√ß√£o de IA.
2. Engenheiros e Cientistas de Dados envolvidos no design, desenvolvimento e teste de sistemas de IA.
3. Auditores e Reguladores que avaliam a conformidade dos sistemas de IA com os padr√µes legais e √©ticos.
4. Lideran√ßa Executiva e Conselhos que precisam tomar decis√µes estrat√©gicas sobre o risco de IA.
5. Pesquisadores e Acad√™micos estudando a governan√ßa e √©tica da IA.

## Usos Pretendidos (Intended Uses)
1. Auxiliar na pesquisa acerca de gerenciamento de riscos de Intelig√™ncia Artificial

## Requisitos Funcionais
1. Quais s√£o os agentes de IA e suas responsabilidades?
2. O que seria risco de uso de um sistema de IA?
3. Como sistemas de IA s√£o classificados quanto ao risco de uso?
4. Como uma sa√≠da gerada por sistema de IA poderia afetar um usu√°rio?
5. Como minimizar um risco gerado por um sistema de IA?
6. Quando avaliar o risco de um sistema de IA?
7. Como avalia√ß√£o √© realizada? (Organiza√ß√£o dos atores e o que fazem)

## Requisitos N√£o-Funcionais
1. A ontologia deve representar a classifica√ß√£o de sistemas de IA em diferentes categorias de risco.
2. A ontologia deve permitir a associa√ß√£o de um risco a um ou mais componentes do sistema de IA (por exemplo, dados de treinamento enviesados).

## Referencias
- [Truly Risk-based Regulation of Artificial Intelligence How to Implement the EU‚Äôs AI Act](https://www.cambridge.org/core/journals/european-journal-of-risk-regulation/article/truly-riskbased-regulation-of-artificial-intelligence-how-to-implement-the-eus-ai-act/E526C1D0D7368F9691082220609D60F4)
- [Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and Regulatory Measures in the EU AI Act](https://arxiv.org/abs/2404.11476)
- [Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
- [The EU Artificial Intelligence Act](https://artificialintelligenceact.eu)